{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%session_id_prefix orders_cdc_upsert_iceberg_01\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\",\n  \"--job-bookmark-option\": \"job-bookmark-enable\",\n  \"--JOB_NAME\": \"orders_cdc_upsert_iceberg\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nSetting session ID prefix to orders_cdc_upsert_iceberg_01\nSetting Glue version to: 3.0\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg', '--job-bookmark-option': 'job-bookmark-enable', '--JOB_NAME': 'orders_cdc_upsert_iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 474053ef-fb15-46bb-b543-2020dec32c12\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n--datalake-formats iceberg\n--job-bookmark-option job-bookmark-enable\n--JOB_NAME orders_cdc_upsert_iceberg\nWaiting for session 474053ef-fb15-46bb-b543-2020dec32c12 to get into ready status...\nSession 474053ef-fb15-46bb-b543-2020dec32c12 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"chiholee-datalake001\"\ndatabase_name = \"ecommerce\"\n\ntable_name = \"orders\"\npk = 'order_id'\nlast_update_time = 'order_dt'\n\nsource_bucket_prefix = \"transaction/cdc/raw\"\nsource_path = f\"s3://{bucket_name}/{source_bucket_prefix}\"\nsource_table_name = table_name\n\niceberg_bucket_prefix = \"transaction/iceberg/glue\"\nwarehouse_path = f\"s3://{bucket_name}/{iceberg_bucket_prefix}\"\niceberg_table_name = f\"{table_name}_cdc_glue_iceberg\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\n\n\nglueContext = GlueContext(spark)\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDyf = glueContext.create_dynamic_frame_from_options(\n    connection_type='s3',\n    connection_options={\n        'paths': [f'{source_path}/{database_name}/{source_table_name}/'],\n        'groupFiles': 'none',\n        'recurse': True\n    },\n    format='parquet',\n    transformation_ctx='cdcDyf')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(f\"## Count of CDC data after last job bookmark:{cdcDyf.count()}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "## Count of CDC data after last job bookmark:132037\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf = cdcDyf.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+--------------------+--------+--------+---------+-----------+-------------------+-----------+----------+\n| Op|     dms_update_time|order_id|promo_id|order_cnt|order_price|           order_dt|customer_id|product_id|\n+---+--------------------+--------+--------+---------+-----------+-------------------+-----------+----------+\n|  I|2024-04-24 19:12:...|  297263| PROMO01|        5|      16000|2024-04-24 19:12:29|         51|         9|\n|  I|2024-04-24 19:12:...|  297264| PROMO03|        2|      30000|2024-04-24 19:12:30|         77|        14|\n|  I|2024-04-24 19:12:...|  297265| PROMO20|        6|      30000|2024-04-24 19:12:30|         38|         9|\n|  I|2024-04-24 19:12:...|  297266| PROMO02|        8|      14000|2024-04-24 19:12:30|         92|         3|\n|  I|2024-04-24 19:12:...|  297267| PROMO06|        9|      17000|2024-04-24 19:12:31|          9|        15|\n|  I|2024-04-24 19:12:...|  297268| PROMO17|       10|      33000|2024-04-24 19:12:32|         86|         7|\n|  U|2024-04-24 19:12:...|  222450| PROMO19|        3|       9000|2024-04-24 19:12:32|         54|         9|\n|  I|2024-04-24 19:12:...|  297269| PROMO12|        1|      14000|2024-04-24 19:12:34|         53|         4|\n|  I|2024-04-24 19:12:...|  297270| PROMO14|        8|      34000|2024-04-24 19:12:34|         20|        13|\n|  I|2024-04-24 19:12:...|  297271| PROMO18|        5|      34000|2024-04-24 19:12:35|         72|        14|\n|  I|2024-04-24 19:12:...|  297272| PROMO10|        5|      23000|2024-04-24 19:12:37|         39|         8|\n|  U|2024-04-24 19:12:...|  173423| PROMO05|        9|      34000|2024-04-24 19:12:41|         43|         3|\n|  I|2024-04-24 19:12:...|  297273| PROMO07|        1|      38000|2024-04-24 19:12:45|         24|        17|\n|  I|2024-04-24 19:12:...|  297274| PROMO15|        4|      19000|2024-04-24 19:12:47|         95|         2|\n|  U|2024-04-24 19:12:...|  242121| PROMO03|       10|      18000|2024-04-24 19:12:48|         74|        13|\n|  I|2024-04-24 19:12:...|  297275| PROMO17|        1|      30000|2024-04-24 19:12:48|         27|         5|\n|  I|2024-04-24 19:12:...|  297276| PROMO17|        1|      15000|2024-04-24 19:12:49|         41|         5|\n|  I|2024-04-24 19:12:...|  297277| PROMO01|        4|      44000|2024-04-24 19:12:50|         89|        16|\n|  I|2024-04-24 19:12:...|  297278| PROMO01|        5|       5000|2024-04-24 19:12:52|         50|         4|\n|  I|2024-04-24 19:12:...|  297279| PROMO14|        8|      36000|2024-04-24 19:12:53|         91|        16|\n+---+--------------------+--------+--------+---------+-----------+-------------------+-----------+----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as F ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf.createOrReplaceTempView(\"cdcDf\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf = spark.sql(\"\"\"\nselect *\nfrom cdcDf\nwhere (order_id, order_dt) in\n(\n    select order_id, max(order_dt) max_op_time\n    from cdcDf\n    group by order_id\n)\n\"\"\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcInsertCount = cdcDf.filter(\"Op = 'I'\").count()\ncdcUpdateCount = cdcDf.filter(\"Op = 'U'\").count()\ncdcDeleteCount = cdcDf.filter(\"Op = 'D'\").count()\nprint(f\"Inserted count: {cdcInsertCount}\")\nprint(f\"Updated count: {cdcUpdateCount}\")\nprint(f\"Deleted count: {cdcDeleteCount}\")\nprint(f\"Total CDC count: {cdcDf.count()}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "Inserted count: 116847\nUpdated count: 12887\nDeleted count: 0\nTotal CDC count: 129734\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dropColumnList = ['Op','dms_update_time']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.functions import concat, col, lit, to_timestamp\n\ncurrent_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\ncdcDf = cdcDf.withColumn('order_dt',to_timestamp(col('order_dt')))\ncdcDf = (cdcDf\n      .withColumn('year', year(col('order_dt')))\n      .withColumn('month', month(col('order_dt')))\n      .withColumn('day', dayofmonth(col('order_dt')))\n     )\ncdcDf = cdcDf.withColumn('last_applied_date',to_timestamp(lit(current_datetime)))\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\")\nexisting_tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{database_name};\")\ndf_existing_tables = existing_tables.select('tableName').rdd.flatMap(lambda x:x).collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "upsertDf = cdcDf.filter(\"Op != 'D'\").drop(*dropColumnList)\nupsertDf.createOrReplaceTempView(f\"{source_table_name}_upsert\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "upsertDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\n|order_id|promo_id|order_cnt|order_price|           order_dt|customer_id|product_id|year|month|day|  last_applied_date|\n+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\n|  297267| PROMO06|        9|      17000|2024-04-24 19:12:31|          9|        15|2024|    4| 24|2024-04-27 05:07:25|\n|  297280| PROMO08|        2|      46000|2024-04-24 19:12:54|        100|         4|2024|    4| 24|2024-04-27 05:07:25|\n|  297284| PROMO03|        5|      26000|2024-04-24 19:12:58|         40|        19|2024|    4| 24|2024-04-27 05:07:25|\n|  301288| PROMO12|        8|      47000|2024-04-24 21:18:11|         19|         5|2024|    4| 24|2024-04-27 05:07:25|\n|  301294| PROMO16|        5|      33000|2024-04-24 21:18:19|          4|         7|2024|    4| 24|2024-04-27 05:07:25|\n|  301317| PROMO13|        3|      33000|2024-04-24 21:18:49|          4|         4|2024|    4| 24|2024-04-27 05:07:25|\n|  301320| PROMO10|        4|       7000|2024-04-24 21:18:53|         14|        20|2024|    4| 24|2024-04-27 05:07:25|\n|  301322| PROMO09|        1|      13000|2024-04-24 21:18:58|         38|         6|2024|    4| 24|2024-04-27 05:07:25|\n|  314359| PROMO08|        1|      14000|2024-04-25 04:11:13|         66|         2|2024|    4| 25|2024-04-27 05:07:25|\n|  314368| PROMO13|        6|      30000|2024-04-25 04:11:20|         47|        18|2024|    4| 25|2024-04-27 05:07:25|\n|  318648| PROMO20|        2|      33000|2024-04-25 06:24:11|         21|        14|2024|    4| 25|2024-04-27 05:07:25|\n|  318658| PROMO07|        1|      10000|2024-04-25 06:24:27|         53|        15|2024|    4| 25|2024-04-27 05:07:25|\n|  318668| PROMO13|        4|      50000|2024-04-25 06:24:43|         51|         2|2024|    4| 25|2024-04-27 05:07:25|\n|  318670| PROMO02|        1|      10000|2024-04-25 06:24:45|         62|         4|2024|    4| 25|2024-04-27 05:07:25|\n|  318676| PROMO05|        5|      22000|2024-04-25 06:24:48|         64|        12|2024|    4| 25|2024-04-27 05:07:25|\n|  333195| PROMO05|        2|      47000|2024-04-25 14:03:58|         37|         6|2024|    4| 25|2024-04-27 05:07:25|\n|  333202| PROMO14|        8|      36000|2024-04-25 14:04:04|         22|         2|2024|    4| 25|2024-04-27 05:07:25|\n|  333232| PROMO07|        2|      10000|2024-04-25 14:04:42|         98|        16|2024|    4| 25|2024-04-27 05:07:25|\n|  352010| PROMO12|        9|      15000|2024-04-25 23:58:51|         82|         6|2024|    4| 25|2024-04-27 05:07:25|\n|  366962| PROMO05|        1|      29000|2024-04-26 07:52:04|         17|         4|2024|    4| 26|2024-04-27 05:07:25|\n+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.sql(f\"\"\"\n# select order_id, count(*)\n# from {catalog_name}.{database_name}.{iceberg_table_name}\n# group by order_id\n# having count(*) > 1\n# \"\"\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.sql(f\"\"\"\n# select order_id, count(*)\n# from {source_table_name}_upsert\n# group by order_id\n# having count(*) > 1\n# \"\"\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "deleteDf = cdcDf.filter(\"Op = 'D'\").drop(*dropColumnList)\ndeleteDf.createOrReplaceTempView(f\"{source_table_name}_delete\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# deleteDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(f\"Table {source_table_name}_iceberg is upserting...\")\nspark.sql(f\"\"\"MERGE INTO {catalog_name}.{database_name}.{iceberg_table_name} t\n    USING {source_table_name}_upsert s ON s.{pk} = t.{pk}\n    WHEN MATCHED THEN UPDATE SET *\n    WHEN NOT MATCHED THEN INSERT *\n    \"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "Table orders_iceberg is upserting...\nDataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"\nselect min(order_dt), max(order_dt)\nfrom {catalog_name}.{database_name}.{iceberg_table_name}\n\"\"\").show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+-------------------+\n|      min(order_dt)|      max(order_dt)|\n+-------------------+-------------------+\n|2024-04-18 01:42:43|2024-04-27 05:05:28|\n+-------------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}