{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-iceberg-dataframe-02\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\",\n  \"--job-bookmark-option\": \"job-bookmark-enable\",\n  \"--JOB_NAME\": \"glue_notebook_cdc_upsert_iceberg\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nSetting session ID prefix to native-iceberg-dataframe-02\nSetting Glue version to: 3.0\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg', '--job-bookmark-option': 'job-bookmark-enable', '--JOB_NAME': 'glue_notebook_cdc_upsert_iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 78c45835-6a4d-428a-a08e-ed42a8a7b71e\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n--datalake-formats iceberg\n--job-bookmark-option job-bookmark-enable\n--JOB_NAME glue_notebook_cdc_upsert_iceberg\nWaiting for session 78c45835-6a4d-428a-a08e-ed42a8a7b71e to get into ready status...\nSession 78c45835-6a4d-428a-a08e-ed42a8a7b71e has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"chiholee-datalake001\"\ndatabase_name = \"ecommerce\"\n\nsource_bucket_prefix = \"transaction/cdc/raw\"\nsource_path = f\"s3://{bucket_name}/{source_bucket_prefix}\"\nsource_table_name = \"orders\"\n\niceberg_bucket_prefix = \"transaction/iceberg\"\nwarehouse_path = f\"s3://{bucket_name}/{iceberg_bucket_prefix}\"\niceberg_table_name = \"orders_cdc_iceberg\"\n\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "pk = 'order_id'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\n\n\nglueContext = GlueContext(spark)\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDyf = glueContext.create_dynamic_frame_from_options(\n    connection_type='s3',\n    connection_options={\n        'paths': [f'{source_path}/{database_name}/{source_table_name}/'],\n        'groupFiles': 'none',\n        'recurse': True\n    },\n    format='parquet',\n    transformation_ctx='cdcDyf')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(f\"## Count of CDC data after last job bookmark:{cdcDyf.count()}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "cdcDf = cdcDyf.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# cdcDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------------------+--------+--------+---------+-----------+-------------------+-----------+----------+\n| Op|    dms_update_time|order_id|promo_id|order_cnt|order_price|           order_dt|customer_id|product_id|\n+---+-------------------+--------+--------+---------+-----------+-------------------+-----------+----------+\n|  I|2024-04-26 12:02:57|  303311| PROMO20|        9|       8000|2024-04-24 22:23:43|         81|         9|\n|  I|2024-04-26 12:02:57|  303312| PROMO06|        5|      15000|2024-04-24 22:23:45|         70|        18|\n|  I|2024-04-26 12:02:57|  303313| PROMO02|        6|      50000|2024-04-24 22:23:45|         84|        15|\n|  I|2024-04-26 12:02:57|  303314| PROMO06|        3|      31000|2024-04-24 22:23:47|          3|         2|\n|  U|2024-04-26 12:02:57|  237501| PROMO20|        9|      29000|2024-04-24 22:23:49|         88|         1|\n|  I|2024-04-26 12:02:57|  303315| PROMO12|        8|      18000|2024-04-24 22:23:49|         81|         1|\n|  I|2024-04-26 12:02:57|  303316| PROMO07|        8|      12000|2024-04-24 22:23:50|         22|         4|\n|  I|2024-04-26 12:02:57|  303317| PROMO03|        7|      12000|2024-04-24 22:23:51|         27|        17|\n|  I|2024-04-26 12:02:57|  303318| PROMO02|       10|      24000|2024-04-24 22:23:51|          5|         8|\n|  I|2024-04-26 12:02:57|  303319| PROMO12|        5|      46000|2024-04-24 22:23:52|          8|         4|\n|  I|2024-04-26 12:02:57|  303320| PROMO08|        3|      49000|2024-04-24 22:23:52|         45|         4|\n|  I|2024-04-26 12:02:57|  303321| PROMO14|        1|      17000|2024-04-24 22:23:52|         84|        12|\n|  I|2024-04-26 12:02:57|  303322| PROMO04|        3|      31000|2024-04-24 22:23:53|         46|        11|\n|  U|2024-04-26 12:02:57|   20312| PROMO11|        9|      26000|2024-04-24 22:23:54|         47|        17|\n|  I|2024-04-26 12:02:57|  303323| PROMO02|        9|      15000|2024-04-24 22:23:57|         67|        16|\n|  I|2024-04-26 12:02:57|  303324| PROMO13|        5|      26000|2024-04-24 22:23:57|         33|        10|\n|  I|2024-04-26 12:02:57|  303325| PROMO20|       10|      16000|2024-04-24 22:23:59|         13|        11|\n|  I|2024-04-26 12:02:57|  303326| PROMO18|        1|      32000|2024-04-24 22:24:00|         81|         4|\n|  I|2024-04-26 12:02:57|  303327| PROMO10|        5|      39000|2024-04-24 22:24:02|         10|         6|\n|  I|2024-04-26 12:02:57|  303328| PROMO19|        4|      19000|2024-04-24 22:24:03|         37|         9|\n+---+-------------------+--------+--------+---------+-----------+-------------------+-----------+----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as F ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 55,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf.createOrReplaceTempView(\"cdcDf\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 90,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf = spark.sql(\"\"\"\nselect *\nfrom cdcDf\nwhere (order_id, order_dt) in\n(\n    select order_id, max(order_dt) max_op_time\n    from cdcDf\n    group by order_id\n)\n\"\"\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 91,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcInsertCount = cdcDf.filter(\"Op = 'I'\").count()\ncdcUpdateCount = cdcDf.filter(\"Op = 'U'\").count()\ncdcDeleteCount = cdcDf.filter(\"Op = 'D'\").count()\nprint(f\"Inserted count: {cdcInsertCount}\")\nprint(f\"Updated count: {cdcUpdateCount}\")\nprint(f\"Deleted count: {cdcDeleteCount}\")\nprint(f\"Total CDC count: {cdcDf.count()}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "dropColumnList = ['Op','dms_update_time']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 94,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.functions import concat, col, lit, to_timestamp\n\ncurrent_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\ncdcDf = cdcDf.withColumn('order_dt',to_timestamp(col('order_dt')))\ncdcDf = (cdcDf\n      .withColumn('year', year(col('order_dt')))\n      .withColumn('month', month(col('order_dt')))\n      .withColumn('day', dayofmonth(col('order_dt')))\n     )\ncdcDf = cdcDf.withColumn('last_applied_date',to_timestamp(lit(current_datetime)))\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 95,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\")\nexisting_tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{database_name};\")\ndf_existing_tables = existing_tables.select('tableName').rdd.flatMap(lambda x:x).collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 96,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "upsertDf = cdcDf.filter(\"Op != 'D'\").drop(*dropColumnList)\nupsertDf.createOrReplaceTempView(f\"{source_table_name}_upsert\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 97,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# upsertDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 98,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\n|order_id|promo_id|order_cnt|order_price|           order_dt|customer_id|product_id|year|month|day|  last_applied_date|\n+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\n|  303326| PROMO18|        1|      32000|2024-04-24 22:24:00|         81|         4|2024|    4| 24|2024-04-26 12:54:54|\n|  303334| PROMO15|        7|      30000|2024-04-24 22:24:11|         34|        10|2024|    4| 24|2024-04-26 12:54:54|\n|   62958| PROMO12|        5|      24000|2024-04-24 22:24:15|         24|         9|2024|    4| 24|2024-04-26 12:54:54|\n|  303339| PROMO14|        5|      29000|2024-04-24 22:24:21|         70|         6|2024|    4| 24|2024-04-26 12:54:54|\n|  303348| PROMO11|        5|      35000|2024-04-24 22:24:38|         66|         7|2024|    4| 24|2024-04-26 12:54:54|\n|  303350| PROMO06|        9|       9000|2024-04-24 22:24:42|         87|         3|2024|    4| 24|2024-04-26 12:54:54|\n|  303700| PROMO05|        2|      31000|2024-04-24 22:36:03|         63|        20|2024|    4| 24|2024-04-26 12:54:54|\n|  303717| PROMO13|        4|      47000|2024-04-24 22:36:28|          2|         4|2024|    4| 24|2024-04-26 12:54:54|\n|  303718| PROMO03|        7|      19000|2024-04-24 22:36:29|         73|         4|2024|    4| 24|2024-04-26 12:54:54|\n|  303728| PROMO11|        2|      37000|2024-04-24 22:36:42|         52|        13|2024|    4| 24|2024-04-26 12:54:54|\n|  303737| PROMO13|        9|       8000|2024-04-24 22:36:52|         97|        18|2024|    4| 24|2024-04-26 12:54:54|\n|  307834| PROMO15|        9|      38000|2024-04-25 00:45:29|         95|        20|2024|    4| 25|2024-04-26 12:54:54|\n|  307851| PROMO10|        1|      15000|2024-04-25 00:45:46|         84|        16|2024|    4| 25|2024-04-26 12:54:54|\n|  308695| PROMO20|        1|      10000|2024-04-25 01:12:44|         94|         8|2024|    4| 25|2024-04-26 12:54:54|\n|  308728| PROMO18|        2|      26000|2024-04-25 01:13:28|         77|         9|2024|    4| 25|2024-04-26 12:54:54|\n|  172502| PROMO19|       10|      44000|2024-04-25 04:15:41|         73|        13|2024|    4| 25|2024-04-26 12:54:54|\n|  314499| PROMO05|       10|      23000|2024-04-25 04:15:43|         70|         1|2024|    4| 25|2024-04-26 12:54:54|\n|  314503| PROMO15|        9|      49000|2024-04-25 04:15:47|         48|        20|2024|    4| 25|2024-04-26 12:54:54|\n|  315060| PROMO16|        5|      26000|2024-04-25 04:31:43|         85|         6|2024|    4| 25|2024-04-26 12:54:54|\n|  315071| PROMO07|       10|      34000|2024-04-25 04:31:59|         20|         4|2024|    4| 25|2024-04-26 12:54:54|\n+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.sql(f\"\"\"\n# select order_id, count(*)\n# from {catalog_name}.{database_name}.{iceberg_table_name}\n# group by order_id\n# having count(*) > 1\n# \"\"\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 99,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------+\n|order_id|count(1)|\n+--------+--------+\n+--------+--------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.sql(f\"\"\"\n# select order_id, count(*)\n# from {source_table_name}_upsert\n# group by order_id\n# having count(*) > 1\n# \"\"\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 100,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------+\n|order_id|count(1)|\n+--------+--------+\n+--------+--------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "deleteDf = cdcDf.filter(\"Op = 'D'\").drop(*dropColumnList)\ndeleteDf.createOrReplaceTempView(f\"{source_table_name}_delete\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 101,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# deleteDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 102,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------+---------+-----------+--------+-----------+----------+----+-----+---+-----------------+\n|order_id|promo_id|order_cnt|order_price|order_dt|customer_id|product_id|year|month|day|last_applied_date|\n+--------+--------+---------+-----------+--------+-----------+----------+----+-----+---+-----------------+\n+--------+--------+---------+-----------+--------+-----------+----------+----+-----+---+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(f\"Table {source_table_name}_iceberg is upserting...\")\nspark.sql(f\"\"\"MERGE INTO {catalog_name}.{database_name}.{iceberg_table_name} t\n    USING {source_table_name}_upsert s ON s.{pk} = t.{pk}\n    WHEN MATCHED THEN UPDATE SET *\n    WHEN NOT MATCHED THEN INSERT *\n    \"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 103,
			"outputs": [
				{
					"name": "stdout",
					"text": "Table orders_iceberg is upserting...\nDataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"\nselect min(order_dt), max(order_dt)\nfrom {catalog_name}.{database_name}.{iceberg_table_name}\n\"\"\").show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 105,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+-------------------+\n|      min(order_dt)|      max(order_dt)|\n+-------------------+-------------------+\n|2024-04-18 01:42:43|2024-04-26 11:59:30|\n+-------------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}