{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-iceberg-dataframe-\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nSetting session ID prefix to native-iceberg-dataframe-\nSetting Glue version to: 3.0\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"chiholee-datalake001\"\ndatabase_name = \"ecommerce\"\n\nsource_bucket_prefix = \"transaction/initial/raw\"\nsource_path = f\"s3://{bucket_name}/{source_bucket_prefix}\"\nsource_table_name = \"orders\"\n\niceberg_bucket_prefix = \"transaction/iceberg\"\nwarehouse_path = f\"s3://{bucket_name}/{iceberg_bucket_prefix}\"\niceberg_table_name = \"orders_cdc_iceberg\"\n\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.context import GlueContext",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "glueContext = GlueContext(spark)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(f'{source_path}/{database_name}/{source_table_name}/')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "s3://chiholee-datalake001/transaction/initial/raw/ecommerce/orders/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fullDyf = glueContext.create_dynamic_frame_from_options(\n    connection_type='s3',\n    connection_options={\n        'paths': [f'{source_path}/{database_name}/{source_table_name}/'],\n        'groupFiles': 'none',\n        'recurse': True\n    },\n    format='parquet',\n    transformation_ctx='fullDyf')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(f\"Count of data after last job bookmark:{fullDyf.count()}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "Count of data after last job bookmark:288650\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fullDf = fullDyf.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fullDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------+---------+-----------+-------------------+-----------+----------+\n|order_id|promo_id|order_cnt|order_price|           order_dt|customer_id|product_id|\n+--------+--------+---------+-----------+-------------------+-----------+----------+\n|  110485| PROMO03|        8|      47000|2024-04-20 11:45:16|         82|        11|\n|  181343| PROMO07|        2|      34000|2024-04-22 00:58:27|         38|         8|\n|  101099| PROMO09|        3|      43000|2024-04-20 06:52:35|         40|        16|\n|  172154| PROMO01|        4|      32000|2024-04-21 20:07:37|         69|        12|\n|  146144| PROMO17|        5|      32000|2024-04-21 06:35:34|          6|        20|\n|  123105| PROMO08|       10|      24000|2024-04-20 18:23:47|         52|         6|\n|  105389| PROMO11|        4|      25000|2024-04-20 09:06:05|         77|         9|\n|  163394| PROMO03|        5|      23000|2024-04-21 15:33:58|         64|        14|\n|  175538| PROMO03|        6|      19000|2024-04-21 21:53:15|         92|         2|\n|  159873| PROMO15|       10|      26000|2024-04-22 03:00:54|         99|         7|\n|  179282| PROMO05|        1|      13000|2024-04-21 23:53:51|         13|         1|\n|  151050| PROMO04|        2|       6000|2024-04-21 09:07:23|         80|         9|\n|  170850| PROMO10|        7|      22000|2024-04-21 19:28:04|         83|        18|\n|  151897| PROMO03|        5|      14000|2024-04-22 03:10:39|         71|         5|\n|  114986| PROMO13|        9|      42000|2024-04-20 14:09:22|         92|         2|\n|  157977| PROMO20|       10|      14000|2024-04-21 12:47:01|         53|        17|\n|  162413| PROMO17|        8|      24000|2024-04-21 15:03:55|          9|         9|\n|  134417| PROMO12|        2|      16000|2024-04-21 00:21:56|         72|         4|\n|  156702| PROMO20|        7|      23000|2024-04-21 12:06:43|         78|         1|\n|  183217| PROMO04|        5|      49000|2024-04-22 01:55:21|         40|        10|\n+--------+--------+---------+-----------+-------------------+-----------+----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.functions import concat, col, lit, to_timestamp\n\ncurrent_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\nfullDf = fullDf.withColumn('order_dt',to_timestamp(col('order_dt')))\nfullDf = (fullDf\n      .withColumn('year', year(col('order_dt')))\n      .withColumn('month', month(col('order_dt')))\n      .withColumn('day', dayofmonth(col('order_dt')))\n     )\nfullDf = fullDf.withColumn('last_applied_date',to_timestamp(lit(current_datetime)))\n\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Incoming records violate the writer assumption that records are clustered by spec and by partition within each spec. Either cluster the incoming records or switch to fanout writers.\n# 아래 insert select 시 위의 에러가 발생하여 파티션 & 정렬함\nfullDf = fullDf.repartition(\"year\", \"month\", \"day\").sortWithinPartitions(\"year\", \"month\", \"day\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 72,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fullDf.createOrReplaceTempView(f\"{source_table_name}_initial\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 73,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fullDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 74,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\n|order_id|promo_id|order_cnt|order_price|           order_dt|customer_id|product_id|year|month|day|  last_applied_date|\n+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\n|  281230| PROMO08|        8|      25000|2024-04-24 05:15:36|         96|         1|2024|    4| 24|2024-04-26 07:00:05|\n|  277880| PROMO03|        4|      38000|2024-04-24 03:31:24|         71|         4|2024|    4| 24|2024-04-26 07:00:05|\n|  280276| PROMO18|        2|      17000|2024-04-24 04:44:53|         22|        12|2024|    4| 24|2024-04-26 07:00:05|\n|  286657| PROMO01|        5|       7000|2024-04-24 13:38:47|         99|        14|2024|    4| 24|2024-04-26 07:00:05|\n|  272537| PROMO11|        6|      44000|2024-04-24 00:42:58|         29|        12|2024|    4| 24|2024-04-26 07:00:05|\n|  277759| PROMO08|        7|      26000|2024-04-24 03:26:35|         48|         4|2024|    4| 24|2024-04-26 07:00:05|\n|  285557| PROMO11|        8|      31000|2024-04-24 07:30:27|         87|         4|2024|    4| 24|2024-04-26 07:00:05|\n|  281271| PROMO10|        3|      34000|2024-04-24 05:16:57|         58|         4|2024|    4| 24|2024-04-26 07:00:05|\n|  283090| PROMO08|        5|      29000|2024-04-24 06:12:49|         62|         8|2024|    4| 24|2024-04-26 07:00:05|\n|  277910| PROMO16|        7|       8000|2024-04-24 03:32:28|         98|         5|2024|    4| 24|2024-04-26 07:00:05|\n|  274728| PROMO20|        9|      49000|2024-04-24 01:50:38|         99|        10|2024|    4| 24|2024-04-26 07:00:05|\n|  280094| PROMO02|        2|       6000|2024-04-24 04:39:47|         43|        12|2024|    4| 24|2024-04-26 07:00:05|\n|  273354| PROMO04|        8|      28000|2024-04-24 01:07:25|         96|         4|2024|    4| 24|2024-04-26 07:00:05|\n|  271572| PROMO14|        5|      17000|2024-04-24 00:13:05|         26|        20|2024|    4| 24|2024-04-26 07:00:05|\n|  288325| PROMO07|       10|      50000|2024-04-24 14:31:40|         98|        20|2024|    4| 24|2024-04-26 07:00:05|\n|  278547| PROMO11|        1|      47000|2024-04-24 03:51:12|         99|        11|2024|    4| 24|2024-04-26 07:00:05|\n|  277199| PROMO02|        8|      47000|2024-04-24 03:08:58|         41|        16|2024|    4| 24|2024-04-26 07:00:05|\n|  288275| PROMO18|        6|      26000|2024-04-24 14:30:17|         37|         6|2024|    4| 24|2024-04-26 07:00:05|\n|  278165| PROMO14|        4|      16000|2024-04-24 03:40:28|         30|        20|2024|    4| 24|2024-04-26 07:00:05|\n|  279221| PROMO17|        2|      15000|2024-04-24 04:12:32|         21|         4|2024|    4| 24|2024-04-26 07:00:05|\n+--------+--------+---------+-----------+-------------------+-----------+----------+----+-----+---+-------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "existing_tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{database_name};\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_existing_tables = existing_tables.select('tableName').rdd.flatMap(lambda x:x).collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{iceberg_table_name}\n            USING iceberg \n            TBLPROPERTIES ('format-version'='2')\n            as (SELECT * from {source_table_name}_initial)\"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{iceberg_table_name}4 (\n  order_id int,\n  promo_id string,\n  order_cnt int,\n  order_price int,\n  order_dt timestamp,\n  customer_id bigint,\n  product_id int,\n  year int,\n  month int,\n  day int,\n  last_applied_date timestamp\n)\nUSING iceberg \nPARTITIONED BY (year, month, day)\"\"\")\n            # as (SELECT * from {source_table_name}_initial)\"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 75,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"\nINSERT INTO {catalog_name}.{database_name}.{iceberg_table_name}4\nSELECT * from {source_table_name}_initial\"\"\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 76,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{iceberg_table_name}\n            USING iceberg \n            PARTITIONED BY (year, month, day)\n            as (SELECT * from {source_table_name}_initial)\"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 83,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"DROP TABLE {catalog_name}.{database_name}.{iceberg_table_name}\"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 82,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nDROP TABLE IF EXISTS {catalog_name}.{database_name}.{table_name}\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nCREATE DATABASE IF NOT EXISTS {database_name}\n\"\"\"\nspark.sql(query)\n     ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nimport time\n\nut = time.time()\n\ndf_products = spark.createDataFrame(\n    [\n        (\"00001\", \"Heater\", 250, \"Electronics\", ut),\n        (\"00002\", \"Thermostat\", 400, \"Electronics\", ut),\n        (\"00003\", \"Television\", 600, \"Electronics\", ut),\n        (\"00004\", \"Blender\", 100, \"Electronics\", ut),\n        (\"00005\", \"Table\", 150, \"Furniture\", ut)\n    ],\n    [\"product_id\", \"product_name\", \"price\", \"category\", \"updated_at\"],\n)\n\ndf_products.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  250|Electronics|1.7141134651999865E9|\n|     00002|  Thermostat|  400|Electronics|1.7141134651999865E9|\n|     00003|  Television|  600|Electronics|1.7141134651999865E9|\n|     00004|     Blender|  100|Electronics|1.7141134651999865E9|\n|     00005|       Table|  150|  Furniture|1.7141134651999865E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_products.sortWithinPartitions(\"category\") \\\n    .writeTo(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .create()\n     ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.catalog.listTables(database_name)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "[Table(name='accesslog', database='ecommerce', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='orders_cdc_iceberg', database='ecommerce', description=None, tableType=None, isTemporary=False)]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\n     ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_products.createOrReplaceTempView(f\"tmp_{table_name}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nCREATE DATABASE IF NOT EXISTS {database_name}\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nCREATE TABLE {catalog_name}.{database_name}.{table_name}\nUSING iceberg\nAS SELECT * FROM tmp_{table_name}\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nDROP TABLE {catalog_name}.{database_name}.{table_name}\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming, etl and glue_ray. \n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Session\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray session. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# %idle_timeout 2880\n# %glue_version 4.0\n# %worker_type G.1X\n# %number_of_workers 5\n\n# import sys\n# from awsglue.transforms import *\n# from awsglue.utils import getResolvedOptions\n# from pyspark.context import SparkContext\n# from awsglue.context import GlueContext\n# from awsglue.job import Job\n  \n# sc = SparkContext.getOrCreate()\n# glueContext = GlueContext(sc)\n# spark = glueContext.spark_session\n# job = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# dyf = glueContext.create_dynamic_frame.from_catalog(database='database_name', table_name='table_name')\n# dyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# df = dyf.toDF()\n# df.show()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Visualize data with matplotlib\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# import matplotlib.pyplot as plt\n\n# # Set X-axis and Y-axis values\n# x = [5, 2, 8, 4, 9]\n# y = [10, 4, 8, 5, 2]\n  \n# # Create a bar chart \n# plt.bar(x, y)\n  \n# # Show the plot\n# %matplot plt",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# s3output = glueContext.getSink(\n#   path=\"s3://bucket_name/folder_name\",\n#   connection_type=\"s3\",\n#   updateBehavior=\"UPDATE_IN_DATABASE\",\n#   partitionKeys=[],\n#   compression=\"snappy\",\n#   enableUpdateCatalog=True,\n#   transformation_ctx=\"s3output\",\n# )\n# s3output.setCatalogInfo(\n#   catalogDatabase=\"demo\", catalogTableName=\"populations\"\n# )\n# s3output.setFormat(\"glueparquet\")\n# s3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 6bff742e-f05a-4996-aa04-36037a308ac0\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom pyspark.sql.functions import *\nfrom awsglue.dynamicframe import DynamicFrame\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import rank, max, col\n\n\nfrom pyspark.conf import SparkConf",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: c9770581-873d-4116-a3d5-7bcb49d3bac4\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\nWaiting for session c9770581-873d-4116-a3d5-7bcb49d3bac4 to get into ready status...\nSession c9770581-873d-4116-a3d5-7bcb49d3bac4 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "CATALOG=\"iceberg_catalog\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "conf = SparkConf()\nconf.set(f\"spark.sql.catalog.{CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\")\nconf.set(f\"spark.sql.catalog.{CATALOG}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\nconf.set(f\"spark.sql.catalog.{CATALOG}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\nconf.set(f\"spark.sql.catalog.{CATALOG}.lock-impl\", \"org.apache.iceberg.aws.glue.DynamoLockManager\")\nconf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "<pyspark.conf.SparkConf object at 0x7f80c8e24e80>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#3. Set the Spark + Glue context\n# sc = SparkContext(conf=conf)\nsc = SparkContext.getOrCreate(conf=conf);\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "RAW_S3_PATH=\"s3://chiholee-datalake001/transaction/initial/raw\"\nDATABASE=\"ecommerce\"\nTABLE_NAME=\"orders\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# 에러 케이스\n# s3 permission 확인\nfullDyf = glueContext.create_dynamic_frame_from_options(\n    connection_type='s3',\n    connection_options={\n        'paths': [f'{RAW_S3_PATH}/{DATABASE}/{TABLE_NAME}/'],\n        'groupFiles': 'none',\n        'recurse': True\n    },\n    format='parquet',\n    transformation_ctx='fullDyf')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fullDyf.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "288650\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fullDyf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "{\"order_id\": 14729, \"promo_id\": \"PROMO18\", \"order_cnt\": 10, \"order_price\": 22000, \"order_dt\": 2024-04-18 09:24:40.0, \"customer_id\": 59, \"product_id\": 5}\n{\"order_id\": 39050, \"promo_id\": \"PROMO02\", \"order_cnt\": 4, \"order_price\": 37000, \"order_dt\": 2024-04-19 06:38:29.0, \"customer_id\": 70, \"product_id\": 3}\n{\"order_id\": 34951, \"promo_id\": \"PROMO13\", \"order_cnt\": 5, \"order_price\": 41000, \"order_dt\": 2024-04-18 20:02:42.0, \"customer_id\": 95, \"product_id\": 8}\n{\"order_id\": 62228, \"promo_id\": \"PROMO05\", \"order_cnt\": 5, \"order_price\": 27000, \"order_dt\": 2024-04-20 23:19:08.0, \"customer_id\": 52, \"product_id\": 17}\n{\"order_id\": 2873, \"promo_id\": \"PROMO06\", \"order_cnt\": 9, \"order_price\": 49000, \"order_dt\": 2024-04-18 03:14:56.0, \"customer_id\": 41, \"product_id\": 15}\n{\"order_id\": 31017, \"promo_id\": \"PROMO06\", \"order_cnt\": 5, \"order_price\": 38000, \"order_dt\": 2024-04-24 00:03:54.0, \"customer_id\": 53, \"product_id\": 20}\n{\"order_id\": 42684, \"promo_id\": \"PROMO10\", \"order_cnt\": 8, \"order_price\": 26000, \"order_dt\": 2024-04-19 00:12:12.0, \"customer_id\": 77, \"product_id\": 20}\n{\"order_id\": 79657, \"promo_id\": \"PROMO15\", \"order_cnt\": 5, \"order_price\": 35000, \"order_dt\": 2024-04-19 19:32:47.0, \"customer_id\": 20, \"product_id\": 19}\n{\"order_id\": 79256, \"promo_id\": \"PROMO06\", \"order_cnt\": 5, \"order_price\": 41000, \"order_dt\": 2024-04-19 19:19:19.0, \"customer_id\": 2, \"product_id\": 1}\n{\"order_id\": 45884, \"promo_id\": \"PROMO02\", \"order_cnt\": 7, \"order_price\": 49000, \"order_dt\": 2024-04-19 01:52:02.0, \"customer_id\": 32, \"product_id\": 11}\n{\"order_id\": 9568, \"promo_id\": \"PROMO18\", \"order_cnt\": 5, \"order_price\": 35000, \"order_dt\": 2024-04-18 06:44:27.0, \"customer_id\": 15, \"product_id\": 8}\n{\"order_id\": 42673, \"promo_id\": \"PROMO08\", \"order_cnt\": 6, \"order_price\": 22000, \"order_dt\": 2024-04-19 00:11:56.0, \"customer_id\": 73, \"product_id\": 17}\n{\"order_id\": 34599, \"promo_id\": \"PROMO19\", \"order_cnt\": 1, \"order_price\": 42000, \"order_dt\": 2024-04-18 19:51:09.0, \"customer_id\": 39, \"product_id\": 17}\n{\"order_id\": 25281, \"promo_id\": \"PROMO16\", \"order_cnt\": 8, \"order_price\": 35000, \"order_dt\": 2024-04-18 14:59:39.0, \"customer_id\": 14, \"product_id\": 4}\n{\"order_id\": 34889, \"promo_id\": \"PROMO04\", \"order_cnt\": 10, \"order_price\": 25000, \"order_dt\": 2024-04-18 20:00:39.0, \"customer_id\": 74, \"product_id\": 19}\n{\"order_id\": 76223, \"promo_id\": \"PROMO17\", \"order_cnt\": 10, \"order_price\": 42000, \"order_dt\": 2024-04-19 17:43:20.0, \"customer_id\": 96, \"product_id\": 7}\n{\"order_id\": 8387, \"promo_id\": \"PROMO12\", \"order_cnt\": 10, \"order_price\": 31000, \"order_dt\": 2024-04-22 14:04:55.0, \"customer_id\": 32, \"product_id\": 1}\n{\"order_id\": 15321, \"promo_id\": \"PROMO10\", \"order_cnt\": 5, \"order_price\": 17000, \"order_dt\": 2024-04-18 09:43:53.0, \"customer_id\": 56, \"product_id\": 9}\n{\"order_id\": 83286, \"promo_id\": \"PROMO01\", \"order_cnt\": 8, \"order_price\": 24000, \"order_dt\": 2024-04-20 00:09:12.0, \"customer_id\": 38, \"product_id\": 14}\n{\"order_id\": 20395, \"promo_id\": \"PROMO08\", \"order_cnt\": 4, \"order_price\": 23000, \"order_dt\": 2024-04-23 18:24:36.0, \"customer_id\": 53, \"product_id\": 4}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3://chiholee-datalake001/transaction/initial/raw/ecommerce/orders/LOAD00000001.parquet\ns3://chiholee-datalake001/transaction/initial/raw/ecommerce/orders/LOAD00000001.parquet",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}