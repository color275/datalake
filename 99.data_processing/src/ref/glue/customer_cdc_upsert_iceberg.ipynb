{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%session_id_prefix customer_cdc_upsert_iceberg_01\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\",\n  \"--job-bookmark-option\": \"job-bookmark-enable\",\n  \"--JOB_NAME\": \"customer_cdc_upsert_iceberg\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nSetting session ID prefix to customer_cdc_upsert_iceberg_01\nSetting Glue version to: 3.0\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg', '--job-bookmark-option': 'job-bookmark-enable', '--JOB_NAME': 'customer_cdc_upsert_iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: d1ca5e1f-3377-4eb6-9006-edf98589ddeb\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n--datalake-formats iceberg\n--job-bookmark-option job-bookmark-enable\n--JOB_NAME customer_cdc_upsert_iceberg\nWaiting for session d1ca5e1f-3377-4eb6-9006-edf98589ddeb to get into ready status...\nSession d1ca5e1f-3377-4eb6-9006-edf98589ddeb has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"chiholee-datalake001\"\ndatabase_name = \"ecommerce\"\n\ntable_name = \"customer\"\npk = 'customer_id'\nlast_update_time = 'last_update_time'\n\nsource_bucket_prefix = \"transaction/cdc/raw\"\nsource_path = f\"s3://{bucket_name}/{source_bucket_prefix}\"\nsource_table_name = table_name\n\niceberg_bucket_prefix = \"transaction/iceberg\"\nwarehouse_path = f\"s3://{bucket_name}/{iceberg_bucket_prefix}\"\niceberg_table_name = f\"{table_name}_cdc_glue_iceberg\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\n\n\nglueContext = GlueContext(spark)\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDyf = glueContext.create_dynamic_frame_from_options(\n    connection_type='s3',\n    connection_options={\n        'paths': [f'{source_path}/{database_name}/{source_table_name}/'],\n        'groupFiles': 'none',\n        'recurse': True\n    },\n    format='parquet',\n    transformation_ctx='cdcDyf')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(f\"## Count of CDC data after last job bookmark:{cdcDyf.count()}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "## Count of CDC data after last job bookmark:0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf = cdcDyf.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as F ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf.createOrReplaceTempView(\"cdcDf\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcDf = spark.sql(\"\"\"\nselect *\nfrom cdcDf\nwhere (customer_id, last_update_time) in\n(\n    select customer_id, max(last_update_time) max_op_time\n    from cdcDf\n    group by customer_id\n)\n\"\"\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: cannot resolve '`customer_id`' given input columns: []; line 4 pos 7;\n'Project [*]\n+- 'Filter named_struct(customer_id, 'customer_id, last_update_time, 'last_update_time) IN (list#3 [])\n   :  +- 'Aggregate ['customer_id], ['customer_id, 'max('last_update_time) AS max_op_time#2]\n   :     +- 'UnresolvedRelation [cdcDf], [], false\n   +- SubqueryAlias cdcdf\n      +- LogicalRDD false\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cdcInsertCount = cdcDf.filter(\"Op = 'I'\").count()\ncdcUpdateCount = cdcDf.filter(\"Op = 'U'\").count()\ncdcDeleteCount = cdcDf.filter(\"Op = 'D'\").count()\nprint(f\"Inserted count: {cdcInsertCount}\")\nprint(f\"Updated count: {cdcUpdateCount}\")\nprint(f\"Deleted count: {cdcDeleteCount}\")\nprint(f\"Total CDC count: {cdcDf.count()}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: cannot resolve '`Op`' given input columns: []; line 1 pos 0;\n'Filter ('Op = I)\n+- LogicalRDD false\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dropColumnList = ['Op','dms_update_time']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.functions import concat, col, lit, to_timestamp\n\ncurrent_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\ncdcDf = cdcDf.withColumn('order_dt',to_timestamp(col('order_dt')))\ncdcDf = (cdcDf\n      .withColumn('year', year(col('order_dt')))\n      .withColumn('month', month(col('order_dt')))\n      .withColumn('day', dayofmonth(col('order_dt')))\n     )\ncdcDf = cdcDf.withColumn('last_applied_date',to_timestamp(lit(current_datetime)))\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: cannot resolve '`order_dt`' given input columns: [];\n'Project [to_timestamp('order_dt, None) AS order_dt#4]\n+- LogicalRDD false\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\")\nexisting_tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{database_name};\")\ndf_existing_tables = existing_tables.select('tableName').rdd.flatMap(lambda x:x).collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "upsertDf = cdcDf.filter(\"Op != 'D'\").drop(*dropColumnList)\nupsertDf.createOrReplaceTempView(f\"{source_table_name}_upsert\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "upsertDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.sql(f\"\"\"\n# select order_id, count(*)\n# from {catalog_name}.{database_name}.{iceberg_table_name}\n# group by order_id\n# having count(*) > 1\n# \"\"\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.sql(f\"\"\"\n# select order_id, count(*)\n# from {source_table_name}_upsert\n# group by order_id\n# having count(*) > 1\n# \"\"\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "deleteDf = cdcDf.filter(\"Op = 'D'\").drop(*dropColumnList)\ndeleteDf.createOrReplaceTempView(f\"{source_table_name}_delete\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: cannot resolve '`Op`' given input columns: []; line 1 pos 0;\n'Filter ('Op = D)\n+- LogicalRDD false\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# deleteDf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(f\"Table {source_table_name}_iceberg is upserting...\")\nspark.sql(f\"\"\"MERGE INTO {catalog_name}.{database_name}.{iceberg_table_name} t\n    USING {source_table_name}_upsert s ON s.{pk} = t.{pk}\n    WHEN MATCHED THEN UPDATE SET *\n    WHEN NOT MATCHED THEN INSERT *\n    \"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: Table or view not found: customer_upsert; line 1 pos 0;\n'MergeIntoTable ('s.customer_id = 't.customer_id), [updateaction(None)], [insertaction(None)]\n:- SubqueryAlias t\n:  +- SubqueryAlias glue_catalog.ecommerce.customer_cdc_iceberg\n:     +- RelationV2[customer_id#5L, password#6, last_login#7, is_superuser#8, username#9, first_name#10, last_name#11, email#12, is_staff#13, is_active#14, date_joined#15, phone_number#16, age#17, gender#18, address#19, last_update_time#20, name#21, year#22, month#23, day#24, last_applied_date#25] glue_catalog.ecommerce.customer_cdc_iceberg\n+- 'SubqueryAlias s\n   +- 'UnresolvedRelation [customer_upsert], [], false\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"\nselect min(last_update_time), max(last_update_time)\nfrom {catalog_name}.{database_name}.{iceberg_table_name}\n\"\"\").show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------------------+---------------------+\n|min(last_update_time)|max(last_update_time)|\n+---------------------+---------------------+\n|  2023-04-08 11:22:14|  2023-04-12 00:34:33|\n+---------------------+---------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}